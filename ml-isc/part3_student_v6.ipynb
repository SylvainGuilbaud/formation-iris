{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Jupyter Notebook\n",
    "\n",
    "In this section, you will be running code blocks to train machine learning models. Each code block has some explanatory context above it, which you can refer back to if need be. \n",
    "\n",
    "Otherwise, we will explain each of the code blocks out loud, so you don't need to read all of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A: Import Necessary Libraries\n",
    "\n",
    "Let's start by importing our libraries!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # great for storing data in tables\n",
    "import numpy as np # great at doing math on matrices of numbers\n",
    "import matplotlib.pyplot as plt # great at plotting things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load iris module\n",
    "import iris\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll also see this variable appear around the notebook. For the exercises, please backspace this variable and insert the correct answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__ANSWER_HERE__ = None # think of this as a \"fill in the blank\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B: Analyze our Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview the data\n",
    "\n",
    "Before we begin, let's glance back over the data one more time. It's good to understand the columns we're working with when building our models, and from our work in Sections 1 and 2 of class, our data has shifted somewhat from its original state.\n",
    "\n",
    "We'll start by reading in the data from our table into a new Pandas DataFrame, hereafter called `stroke_df`.\n",
    "\n",
    "Running `.head()` on a Pandas DataFrame displays the first 5 rows, including all columns and their respective values. It's a good way to view the DataFrame at a glance.\n",
    "\n",
    "**Exercise:** after the line defining the DataFrame `stroke_df`, run `.head()` on the DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# connect to iris \n",
    "\n",
    "# change these variables to reflect your connection\n",
    "hostname = \"iris\"\n",
    "port = 1972\n",
    "namespace = \"APPL\"\n",
    "username = \"SuperUser\"\n",
    "password = \"SYS\"\n",
    "\n",
    "# connect\n",
    "connection = iris.connect(\"{:}:{:}/{:}\".format(hostname, port, namespace), username, password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"SELECT * FROM SQLUser.stroke_data_processed\"\n",
    "stroke_df = pd.read_sql_query(sql, connection, dtype = None)\n",
    "stroke_df.__ANSWER_HERE__ # fill in the answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C: Prepare our Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assigning `X` and `y` for our models\n",
    "\n",
    "We can think of the output (`y`) as a function of the input (`X`). In other words, whether or not we think a given patient will have a stroke (`y`) based on an assortment of inputs, like their BMI, age and average glucose level (`X`). Here, we break up the `stroke_df` into two subsets `X` and `y`.\n",
    "\n",
    "The _x_ and _y_ symbols are constantly used in math for this same purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = stroke_df[[ # new subset of each patient's attributes that might contribute to their stroke likelihood\n",
    "    'gender', \n",
    "    'age',\n",
    "    'ever_married',\n",
    "    'hypertension', \n",
    "    'heart_disease', \n",
    "    'work_type', \n",
    "    'Residence_type',\n",
    "    'avg_glucose_level', \n",
    "    'bmi',\n",
    "    'smoking_status'\n",
    "]]\n",
    "y = stroke_df['stroke'] # whether or not each patient had a stroke"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting our data for training and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We leave some of our data to *train* the model, and the rest to *test* the model's training.\n",
    "\n",
    "**Exercise**: set the training data be 80% of the original DataFrame (`0.8`), and the `random_state` set to 42."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split # a quick way to randomly split our data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=__ANSWER_HERE__, random_state=__ANSWER_HERE__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run `.head()` again on the `X_test` dataframe to see some of the patients that were included in the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head() # let's see who was included in the testing data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part D: Running our Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost at a glance\n",
    "\n",
    "To begin training a model, let's first initialize a classifier model from [the `xgboost` library](https://xgboost.ai/). Classification models are used to *classify* data into different categories, whereas *regression* models are used to predict numerical values. In our case, we're attempting to classify two states: stroke or non-stroke."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*_Tybk5hymsWuYNDxJ0ZPdg.png\" width=\"80%\">\n",
    "\n",
    "_[via Aviv Nahon on Medium.](https://medium.com/riskified-technology/xgboost-lightgbm-or-catboost-which-boosting-algorithm-should-i-use-e7fda7bb36bc)_\n",
    "\n",
    "XGBoost uses [**gradient boosted decision trees**](https://towardsdatascience.com/a-visual-guide-to-gradient-boosted-trees-8d9ed578b33) to classify data. \n",
    "\n",
    "A **decision tree** has different \"forks in the road\" to send points down the path to classification. For example, a \"fork\" might ask: _\"Does this patient have a high BMI?\"_ If the answer is yes, the model may send that patient down the path towards positive stroke classification. These trees have a maximum number of **layers** the points can traverse down, and multiple trees can be created and compared to one another at once.\n",
    "\n",
    "**Gradient boosting** combines weak learners, like \"stumps\" (trees with only one decision split), so that each new tree corrects the errors of its predecessor. XGBoost employs this method, as do similar libraries like CatBoost and LightGBM.\n",
    "\n",
    "Run the block below to import `xgboost` and its classifier, `XGBClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also need to mark some of the columns as `categorical`, so that our `XGBClassifier` is ready to analyze them as such."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = X_train.select_dtypes(include='object').columns\n",
    "for col in categorical_features:\n",
    "    X_train[col] = X_train[col].astype('category')\n",
    "    X_test[col] = X_test[col].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the model\n",
    "\n",
    "For our first model, let's choose some pretty basic hyperparameters:\n",
    "\n",
    "| argument        | starting value | explanation                                                                                                                                                           |\n",
    "|-----------------|----------------|----------------------------------------------------------------------------|\n",
    "| `n_estimators`  | `2`            | The maximum number of trees that can be built during training.                                                                                                        |\n",
    "| `max_depth`     | `3`            | The maximum depth of the tree.                                                                                                                                        |\n",
    "| `learning_rate` | `0.01`         | How fast the model steps down the gradient. We don't want this to be too high or too low. |\n",
    "\n",
    "The last few hyperparameters are filled in for you. `verbosity` is the level of reporting the model returns on a scale of 0-3, the `random_seed` ensures everyone's model performs the same, and `enable_categorical` allows XGBoost to take in categorical data (`\"Female\"`, `\"Male\"`, `\"Other\"` as opposed to `1`, `2` or `3`).\n",
    "\n",
    "**Exercise:** fill in the missing values for our XGBClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBClassifier(\n",
    "    n_estimators=__ANSWER_HERE__,            # the maximum number of trees to be created during training\n",
    "    max_depth=__ANSWER_HERE__,               # the maximum depth of the tree\n",
    "    learning_rate=__ANSWER_HERE__,           # how fast the model steps down the loss gradient\n",
    "    verbosity=1,\n",
    "    random_state=42,\n",
    "    enable_categorical=True,\n",
    "    tree_method=\"hist\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our model has been instantiated, we can train it using the data we split earlier. If this were a sports movie, this would be the training montage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train) # train our model using our splitted data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has now read read through all patients belonging to the training data and is ready to make its own assessments for unseen patients. To put our model to the test, we feed it patients it's never seen before. If this were a sports movie, this would be the big game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_proba = model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems like a good accuracy, but let's check out the AUC. The `sklearn` library gives us access to a method, `metrics.roc_auc_score()`, that allows us to calculate this easily. Let's try this function on the model we just trained.\n",
    "\n",
    "**Exercise:** call `metrics.roc_auc_score()`, using `y_test` and `preds_proba[:,1]` as the respective arguments. `[:,1]` at the end of `preds_proba` creates a new view of the array, including nothing but the positive scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "metrics.roc_auc_score(__ANSWER_HERE__, __ANSWER_HERE__[:,1]) # let's check out the AUC of our first model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An AUC of ~0.84 is pretty good! However, IRIS Integrated ML performed better at 0.87. Perhaps if we tried some fancier hyperparameters, we could beat that score. \n",
    "\n",
    "Our model is still pretty basic. To demonstrate this, let's see the spread of positive weights it gave to the patients in the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.array(preds_proba[:,1]) # an array of the positive categorization scores the model assigned to each patient\n",
    "n = len(weights)\n",
    "X = weights\n",
    "Y = np.zeros(n)\n",
    "\n",
    "fg = plt.figure()\n",
    "fg.set_figheight(1)\n",
    "plt.scatter(X, Y, marker=\"+\")\n",
    "plt.xticks()\n",
    "plt.yticks(())\n",
    "\n",
    "plt.show() # plot the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's barely any diversity in the risk scores it's giving to each patient. The smallest positive score sits at ~0.4902, and the highest sits at ~0.496. It's not terribly opinionated on what determines a higher likelihood for a stroke, which means we need to tune it more closely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part E: Coding a smarter model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model could use some better hyperparameters. To create a smarter model, let's tune some of our preexisting hyperparameters, and add a few more.\n",
    "\n",
    "**You don't need to understand each hyperparameter,** but you can visit [this link](https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.XGBClassifier) on your own to learn more about `XGBClassifier`'s hyperparameters.\n",
    "\n",
    "These (incredibly) specific values for each hyperparameter were generated automatically during a process called [automated hyperparameter tuning](https://towardsdatascience.com/hyperparameters-optimization-526348bb8e2d)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'n_estimators': 30,                      # the maximum number of trees to be created during training\n",
    "    'max_depth': 5,                          # the maximum depth of each tree\n",
    "    'learning_rate': 0.26634027963287155,    # how fast the model steps down the loss gradient\n",
    "    'subsample': 0.7085736032465515,\n",
    "    'min_child_weight': 14,\n",
    "    'gamma': 1.5174644767193246, \n",
    "    'scale_pos_weight': 2, \n",
    "    'reg_lambda': 0.7417953276343711, \n",
    "    'reg_alpha': 0.5572815198039048, \n",
    "    'colsample_bytree': 0.8805086604399132\n",
    "}\n",
    "\n",
    "fancier_model = XGBClassifier(\n",
    "    enable_categorical=True,\n",
    "    tree_method=\"hist\",\n",
    "    random_state=20230413,\n",
    "    **params\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, let's go through the steps of training and testing our model. This might take a little bit to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fancier_model.fit(X_train, y_train)\n",
    "fancier_preds_class = fancier_model.predict(X_test)\n",
    "fancier_preds_proba = fancier_model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, let's see what percentage of our testing data actually had a stroke. We'll need this to understand the next few cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = X_test.copy()\n",
    "test_df['stroke'] = y_test\n",
    "test_df['probability'] = fancier_preds_proba[:,1]\n",
    "total_stroke = len(test_df.query('stroke == 1'))\n",
    "\n",
    "print(f\"{total_stroke} patients out of {len(test_df)} patients in the test data actually had a stroke.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we compute the AUC of our model, let's see which patients were ranked highest in terms of stroke risk. We'll display the first 41 patients -- with a model with a perfect AUC, the first 41 patients would _all_ be positive cases.\n",
    "\n",
    "From the output of the below cell, pay attention to the last two columns. `stroke` shows whether or not this patient _actually_ had a stroke, and `probability` shows the risk score our model assigned to the patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = X_test.copy()\n",
    "test_df['stroke'] = y_test\n",
    "test_df['probability'] = fancier_preds_proba[:,1]\n",
    "test_df.sort_values('probability', ascending=False, ignore_index=True, inplace=True)\n",
    "test_df.head(total_stroke) # let's see the first 41 rows. how many are actual stroke patients?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance, this doesn't seem like a very good output. There are a few 1's in there, but there are mostly 0's.\n",
    "\n",
    "However, if we eyeball the output of the next cell, we can notice that most of the positive cases are _above_ the halfway point (982 / 2 = 491) in the sorted testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.query('stroke == 1') # where are the real stroke patients? (look at the index column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After eyeballing the positive cases, let's calculate the AUC for our `fancier_model`.\n",
    "\n",
    "**Exercise:** call `metrics.roc_auc_score()` using `y_test` and `fancier_preds_proba[:,1]` as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.roc_auc_score(__ANSWER_HERE__, __ANSWER_HERE__[:,1]) # compute the AUC for our fancier model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that, 89% of the time, a randomly selected stroke class will have a higher positive probability using the ML model than a randomly selected non-stroke class. This is a pretty good AUC score!\n",
    "\n",
    "To see the bigger picture, let's check out the spread of positive scores the model assigned to patients in the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.array(fancier_preds_proba[:,1]) # an array of the positive categorization scores the fancier_model assigned to each patient\n",
    "n = len(weights)\n",
    "X = weights\n",
    "Y = np.zeros(n)\n",
    "\n",
    "fg = plt.figure()\n",
    "fg.set_figheight(1)\n",
    "plt.scatter(X, Y, marker=\"+\")\n",
    "plt.xticks()\n",
    "plt.yticks(())\n",
    "\n",
    "plt.show() # plot the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there is a wide spread of weights between ~0.01 and ~0.70 -- a much nicer spread than our previous model. Our model is much more opinionated!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part F (Bonus): Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see which columns caused the greatest impact on our model's risk scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_scores = fancier_model.feature_importances_\n",
    "\n",
    "# create a bar chart of feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(importance_scores)), importance_scores)\n",
    "plt.xticks(range(len(importance_scores)), X_train.columns)\n",
    "plt.title('Feature Importance')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importance Scores')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part G: Exporting the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All we need to do now is export the model to our machine so that we can load it into the next section, where we will explore ObjectScript Embedded Python!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fancier_model.save_model(\"/tmp/xgboost_export.json\") # save the model for the next section"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
